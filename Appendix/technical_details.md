# DynaMind
In this section, we will delve into the architecture and workflow of DynaMind. As illustrated in the following figure, the system is primarily composed of three main components: *Inference Engine*, *Memory Manager*, and *Operators*. In a nutshell, the intellectual core, *Inference Engine* retrieves the knowledge required for decision-making from *Memory Manager* based on contexts and then generates subsequent instructions, which will be executed by *Operators*. 

![The system overview of DynaMind.](https://github.com/Elfsong/DynaMind/blob/main/Appendix/DynaMind_overview.png)

## Inference Engine
*Inference Engine* is powered by one or more LLMs which possess exceptional natural language reasoning capabilities. By integrating contextual information into carefully crafted prompt templates, DynaMind empowers these LLMs to produce structured instructions readily interpretable by *Operators*. Formally, *Inference Engine* can be formalized as follows:

`IE(O_in, P_in) -> List[(O_out, P_out)]`

Here, `IE` stands for *Inference Engine*, `O_in` denotes the running operator, and `P_in` represents the input parameters associated with the running operator. The output of *Inference Engine* is represented as a list of subsequent command tuples, denoted by `List[(O_out, P_out)]`. Each tuple within the list consists of the output operator `O_out` and its corresponding parameters `P_out`. It is important to note that the output list can be empty, both `O_in` and `O_out` should be constrained within the operator list, and both `P_in` and `P_out` should be correctly parsed by the corresponding operator. Moreover, *Inference Engine* internally employs a FIFO (First In, First Out) priority queue to keep track of the operators that require execution `Queue[(O_in, P_in)]`. If the output is an empty list, i.e., `List[None]`, the engine pops the current operator directly. Otherwise, the engine replaces the current operator with the output list `List[(O_out, P_out)]` and proceeds with the traversal until the queue becomes empty. This recursive solution method can gradually decompose the original query into sub-problems when the system cannot solve the query directly. To handle the potential infinite recursive problem, DynaMind allows users to set a maximum recursion depth, thereby controlling the extent of problem decomposition. 

For instance, let's consider the query: "*The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where?*". The first item in the operator queue will be ("Coordinate", {"query": "*The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where?*"}). The desired output should resemble the case shown in the following figure.

![The pipeline of example "*The 1995 Tooheys 1000 driver who has second-to-last in the Tooheys Top 10 was born where?*".](https://github.com/Elfsong/DynaMind/blob/main/Appendix/DynaMind_pipeline.png)

In light of differences in model parameters and training data, the capabilities of LLMs can differ significantly. Therefore, different LLMs can be employed for various tasks during the inference process. For example, while relatively small-scale LLMs may suffice for text summarization, large-scale LLMs are required for logical reasoning. Rather than relying on a single model, carefully selecting the appropriate model for each task not only enhances inference speeds but also improves overall performance. Currently, DynaMind supports the following LLMs: [OpenAI GPT-3.5 and GPT-4](https://openai.com/blog/openai-api), [Llama series models](https://huggingface.co/huggyllama/llama-30b), and [Falcon series models](https://huggingface.co/tiiuae/falcon-40b).

## Memory Manager

*Memory Manager* plays a pivotal role in storing and organizing the memories of DynaMind. It consists of five interconnected modules: Knowledge Representation, Knowledge Retrieval, Long-term Memory, Short-term Memory, and Knowledge Metabolism. *Memory Manager* works hand in hand with *Inference Engine*, engaging in frequent interactions to ensure a continuous acquisition and updating of the knowledge necessary for inferencing. This section provides an overview of the functionalities within *Memory Manager* and elaborates on their interactions with *Inference Engine*.

**Knowledge Representation** is responsible for encoding knowledge in a format that can be efficiently processed by DynaMind. Typically, there are several common methods used for knowledge representation, such as Bag of Words (BOW) [[1]](#1), Word2Vec [[2]](#2), or fine-tuning a transformer encoder [[3]](#3). To uphold the close alignment with the LLMs' semantic space, we utilized the embeddings generated by LLMs as the knowledge representation. For OpenAI series models, we can directly retrieve the embedding through their official APIs[^1]. For other open-source models, we choose the last hidden state as the representation.

**Knowledge Retrieval** utilizes vector search to construct an index on DynaMind's memory, enabling the rapid identification and retrieval of the most relevant knowledge. By leveraging the Langchain library [[4]](#4), DynaMind can effortlessly access and shift between distinct vector indexing schemes. This module evaluates the relevance of knowledge within the given context, empowering DynaMind to access pertinent knowledge throughout the reasoning process efficiently. In DynaMind's memory, every piece of knowledge is represented as a triple <Context, Key, Value>. The "Context" component houses the contextual data relating to the knowledge. The "Key" signifies the vectorized representation of the "Context", while the "Value" embodies the specific substance of the knowledge.

**Long-term Memory** acts as a permanent knowledge repository within DynaMind, accumulating and retaining large amounts of information over time. The repository holds a collection of various knowledge from a variety of sources, such as previous interactions, external databases, and the Internet. DynaMind harnesses the power of long-term memory to enhance its reasoning ability by integrating past experience and acquired knowledge. Furthermore, users can actively manipulate the knowledge engaged in the inference by explicitly updating the long-term memory, thereby granting them heightened control over DynaMind's cognitive processes.

**Short-term Memory** functions as a temporary workspace for *Inference Engine*. To incorporate external information into the inference process,  all the knowledge stored in short-term memory is consolidated with the user query and provided to *Inference Engine*. However, due to the context length limitation of LLMs, short-term memory primarily retains the immediate knowledge required to support the current reasoning task. To ensure efficient use of short-term memory, each piece of knowledge added is assigned a variable that gradually decreases over time. Once this variable falls below a predefined threshold, the knowledge will be popped from the short-term memory, unless it is recalled again. This mechanism mimics human cognitive behavior during reasoning tasks. It enables DynaMind to adapt to dynamic situations swiftly by selectively retrieving and discarding pertinent knowledge in short-term memory.

**Knowledge Metabolism**
The knowledge stored in long-term memory may expire as time progresses. For instance, the statement "Donald Trump is the current President of the United States" was accurate in 2020, but it became outdated after 2021. This issue is prevalent in real life but discovering and renovating such knowledge necessitates a painstaking endeavor [[5]](#5). To alleviate this predicament, DynaMind is designed to adaptively modulate the credibility of long-term memory knowledge in response to varying contextual conditions. We called this mechanism *Knowledge Metabolism*.

Inspired by the Multi-Armed Bandit (MAB) algorithm, we abstract this issue as an exploration-exploitation problem [[6,7]](#6,#7). During the cold start period, *Memory Manager* retrieves $ K_t $ knowledge pieces. At this stage, the model lacks prior knowledge about the credibility associated with each knowledge piece $ k $. To figure out the most reliable knowledge for the given context $ c $, we assume a linear relationship between the credibility and the context. Consequently, the problem is transformed into a MAB estimation task, wherein we employ contextual features $ v_{t,k} $ of combining both $ k $ and $ c $ to estimate the credibility and its confidence interval. Subsequently, we choose the knowledge piece with the highest upper limit in its confidence interval $ p_{t,k} $. This approach emphasizes exploration when the confidence interval for $ k $ is wide, indicating fewer selections and increased uncertainty, representing the risky component of the algorithm. Conversely, if the confidence interval for $ k $ is narrow, implying an increased certainty regarding their credibility, the algorithm tends to favor knowledge pieces with higher means, reflecting the conservative and cautious side of the algorithm. 

```python
def knowledge_metabolism(alpha):
    for t in range(1, T + 1):
        v_tk = ["contextual features for each k in K_t"]  # placeholder
        for k in K_t:
            if k is new:
                A_k = I_dxd
                b_k = 0_d1
            else:
                theta_k = inv(A_k) @ b_k
                p_tk = theta_k.T @ v_tk[k] + alpha * sqrt((v_tk[k].T @ inv(A_k) @ v_tk[k]))
        
        k_o = argmax(p_tk)  # choose arm with highest upper limit
        r_t = "observe a real-valued payoff"  # placeholder
        A_ko = A_ko + v_tk[k_o] @ v_tk[k_o].T
        b_ko = b_ko + r_t * v_tk[k_o]
```

## Operators
DynaMind provides a set of atomic operators that harness the power of an LLM with well-designed prompt templates. By permuting these essential operators, DynaMind is able to break down intricate problems into smaller, manageable sub-problems recursively, enabling step-by-step resolution. Moreover, the operators incorporate integrated functionalities such as invoking search engines and browsing websites. Below is a table showing the definition of the built-in operators. For specific operator formats and illustrative examples, please refer to Appendix Table with a reference link to [Appendix Table](#tab:operators_details).

|**Name**| **Definition**|
|--- |--- |
|Coordinator| *Context: List[Knowledge: str], Query: str → List[<Operator: str, Args: dict>]*???|
|Searcher| *Query: str → Results: List[Result: dict]*|
|Browser| *Path: str, Query: str → Answer: str*|
|Responder| *Context: List[Knowledge: str], Query: str → Response: str*|
|Discriminator| *Context: List[Knowledge: str], Query: str, Response: str → Validity: boolean*|

*Table: The definition of DynaMind operators.*

**Coordinator** plays a vital role in efficiently managing and coordinating the activities of other operators. As central control operators, they hold the responsibility of recursively breaking down queries into sub-problems based on the context. This breakdown continues until the sub-problems can be handled independently by other operators, enabling efficient collaboration among them to solve complex problems effectively. 

**Searcher** in DynaMind provides both keyword-based and vector-based searches to supply context-related knowledge for *Inference Engine*. By leveraging the keyword-based search, DynaMind offers a set of SQL-like interfaces that enable a fine-grained level of control over the data stored in memory. On the other hand, vector search can capture implicit relationships between context and query, granting the system greater flexibility in retrieving knowledge. As a result, the hybrid search solution allows DynaMind to retrieve knowledge in a more nuanced and accurate manner, taking into account the underlying meaning and connections between different pieces of knowledge.

**Browser** runs as a cohesive operator for reading file resources. Currently, it supports parsing HTML/PDF files and allows to support more extensions through customizing the operator. To facilitate inference by *Inference Engine*, DynaMind handles files whose content exceeds the token limit of LLMs in the following manner: 
1. It employs the boilerplate removal library to eliminate irrelevant content from the files. 
2. It utilizes text summarization techniques to compress the file content into an appropriate length that can be processed effectively.

**Responder** is invoked to consolidate all knowledge in the short-term memory and generate a response if the coordinator operator determines that the current context and acquired knowledge are sufficient to provide an answer to the user's query. It is important to note that the response does not immediately deliver to the user. Instead, it calls upon the discriminator operator to evaluate whether the generated result fulfills the user's requirements. Should the response satisfy the user's expectations, it is subsequently relayed back to the user. Otherwise, DynaMind reserves the unsatisfactory response in its short-term memory to avoid replicating a similar case, and then activates the coordinator to generate a more suitable response.

**Discriminator** evaluates whether the response meets the user's requirements. If the response is satisfactory, the discriminator calculates the contribution of each piece of knowledge in the current short-term memory to the response and increases the credibility of the corresponding knowledge proportionally. On the contrary, if the response does not meet the requirements, the discriminator accordingly lowers their credibility. Knowledge with high credibility will receive more attention in the retrieval phase, while knowledge with low credibility will be ignored.

